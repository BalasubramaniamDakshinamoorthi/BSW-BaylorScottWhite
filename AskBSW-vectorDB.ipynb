{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578430ab-4dc6-4edf-aa53-f3c72d28d7ae",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we will cover the steps required to build the logic that powers AskBSW. The key components are:\n",
    "- Reading in HR Documents\n",
    "- Generating Embeddings for each chunk of HR documents\n",
    "- Upload embeddings into VectorDB\n",
    "- Test VectorDB search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd581fb-ccee-4903-9736-1521a32fa2b0",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "- These packages are needed to parse the HR documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c9c31c-841f-47f8-a77a-6b8747cba4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (3.17.0)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3; python_version < \"3.10\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pypdf) (4.6.0)\n",
      "Requirement already satisfied: python-docx in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from python-docx) (4.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "!pip install python-docx\n",
    "!pip install azure-search-documents==11.4.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d99035-68b4-4879-92b9-f4a13cd4db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from docx import Document\n",
    "import openai\n",
    "import re, sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector \n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0239536-e5df-403f-8af6-3f79622da009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=saaskbswdeveast2;AccountKey=DWFZmBZYho1KSOp77C2jHqGuN5eaPeiOUum1BZnBqGPB62zFoEy16JkxP4ER3fIsgdNOJDSyOM4x+AStZMiHPw==;EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_name = \"datascience\"\n",
    "AZURE_OPENAI_KEY = \"YOUR_OPENAI_KEY\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://llm-dev-02.openai.azure.com/\"\n",
    "API_VERSION = \"2023-05-15\"\n",
    "EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-ada02-askbsw\"\n",
    "openai_client = AzureOpenAI(\n",
    "  api_key = AZURE_OPENAI_KEY,  \n",
    "  api_version = API_VERSION,\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "# Azure Cognitive Search Vector Store Configuration\n",
    "search_service_endpoint: str = \"https://cog-askbsw-dev.search.windows.net/\"\n",
    "search_service_api_key: str = \"YOUR_COGNITIVE_SEARCH_KEY\"\n",
    "index_name: str = \"YOUR_INDEX_NAME\"\n",
    "credential = AzureKeyCredential(search_service_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd30089-1a8e-42d2-b795-f8552b95b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "def process_blobs(blob_service_client, container_name, directory_name=\"AskBSWH/HR\", num_tokens=128, verbose=False):\n",
    "    \"\"\"\n",
    "    Recursively processes blobs in a given directory and its subdirectories \n",
    "    in an Azure Blob Storage container.\n",
    "\n",
    "    Parameters:\n",
    "    - blob_service_client (BlobServiceClient): The Azure Blob Service Client.\n",
    "    - container_name (str): The name of the container.\n",
    "    - directory_name (str, optional): The name of the directory to start with.\n",
    "      Defaults to an empty string, which means the root directory.\n",
    "    - num_tokens (int, optional): The number of tokens for each chunk. Default is 128.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    None: This function prints the contents of each DOCX and PDF file found.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    fnames = []\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_list = container_client.list_blobs(name_starts_with=directory_name)\n",
    "    \n",
    "    for blob in blob_list:\n",
    "        blob_name = blob.name\n",
    "        \n",
    "        if blob_name.endswith('/'):  # This is a \"directory\"\n",
    "            process_blobs(blob_service_client, container_name, blob_name)\n",
    "        else:\n",
    "            blob_client = container_client.get_blob_client(blob_name)\n",
    "            stream = blob_client.download_blob()\n",
    "            bytes_io = BytesIO(stream.readall())\n",
    "            \n",
    "            # Process DOCX files\n",
    "            if blob_name.lower().endswith('.docx'):\n",
    "                doc = Document(bytes_io)\n",
    "                word_count = 0\n",
    "                text = ' '.join(para.text for para in doc.paragraphs)\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Tokenize and chunk the text\n",
    "                tokens = text.split()\n",
    "                for i in range(0, len(tokens), num_tokens):\n",
    "                    chunks.append(' '.join(tokens[i:i + num_tokens]))\n",
    "                    fnames.append(blob_name)\n",
    "                for para in doc.paragraphs:\n",
    "                    word_count += len(para.text.split())\n",
    "                if verbose:\n",
    "                    print (blob_name)\n",
    "                    print(f\"Total number of words in document: {word_count}\")\n",
    "\n",
    "            # Process PDF files\n",
    "            elif blob_name.lower().endswith('.pdf'):\n",
    "                reader = PdfReader(bytes_io)\n",
    "                num_pages = len(reader.pages)\n",
    "                if verbose:\n",
    "                    print (blob_name)\n",
    "                    print (f\"Num pages: {num_pages}\")\n",
    "                for i in range(num_pages):\n",
    "                    page = reader.pages[i]\n",
    "                    text = page.extract_text()\n",
    "                    text = text.replace('\\n', ' ')\n",
    "\n",
    "                    # Tokenize and chunk the text\n",
    "                    tokens = text.split()\n",
    "                    for i in range(0, len(tokens), num_tokens):\n",
    "                        chunks.append(' '.join(tokens[i:i + num_tokens]))\n",
    "                        fnames.append(blob_name) \n",
    "\n",
    "    return chunks, fnames\n",
    "\n",
    "def generate_embeddings(text, model):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text, model=model)\n",
    "    embeddings = response.data[0].embedding\n",
    "    return embeddings\n",
    "\n",
    "def search_vector_db(user_input):\n",
    "    embeddings = generate_embeddings(text=user_input, model=EMBEDDING_DEPLOYMENT_NAME)\n",
    "    vectorList = [Vector(value=embeddings, k=5, fields=\"vector\")]\n",
    "    search_results = search_client.search(user_input, vectors=vectorList, top=3)\n",
    "    \n",
    "    # Save results to a list\n",
    "    saved_results = []\n",
    "    for doc in search_results:\n",
    "        saved_results.append(doc)\n",
    "        print (doc[\"text\"])\n",
    "    return saved_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0d2046-df29-4170-9f60-2127a1bceb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, fnames = process_blobs(blob_service_client, container_name, num_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe9b887f-0852-4491-aead-e78349a9e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 records. Elapsed time: 1523.86 seconds\n"
     ]
    }
   ],
   "source": [
    "embeddings_array = []\n",
    "start_time = time.time()\n",
    "\n",
    "for index, chunk in enumerate(chunks):\n",
    "    time.sleep(3)\n",
    "    embedding_chunk = generate_embeddings(chunk, model=EMBEDDING_DEPLOYMENT_NAME)\n",
    "    embeddings_array.append(embedding_chunk)\n",
    "    \n",
    "    if (index + 1) % 500 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {index + 1} records. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a85b9-afdb-4b2c-a1d1-d43815607559",
   "metadata": {},
   "source": [
    "## Upload embeddings to cognitive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "568edf5c-0338-4543-b1e2-68085f2065c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector_id</th>\n",
       "      <th>vector</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.012635222636163235, 0.01425582729279995, -...</td>\n",
       "      <td>0</td>\n",
       "      <td>Page 1 of 2 Title: Adoption Assistance Departm...</td>\n",
       "      <td>AskBSWH/HR/Benefits/Policies &amp; Procedures/Adop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.014502687379717827, 0.011701651848852634, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption Assistance BSWH. HR.BNFT.001.P Page 2...</td>\n",
       "      <td>AskBSWH/HR/Benefits/Policies &amp; Procedures/Adop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.019268624484539032, 0.008453466929495335, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sCOPE This document applies to the Baylor Scot...</td>\n",
       "      <td>AskBSWH/HR/Benefits/Policies &amp; Procedures/Empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.02202620729804039, -0.024142025038599968, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>qualifying incidents occurring after the emplo...</td>\n",
       "      <td>AskBSWH/HR/Benefits/Policies &amp; Procedures/Empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.023361289873719215, -0.009505768306553364,...</td>\n",
       "      <td>4</td>\n",
       "      <td>sCOPE This document applies to Baylor Scott &amp; ...</td>\n",
       "      <td>AskBSWH/HR/Benefits/Policies &amp; Procedures/Tuit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vector_id                                             vector  id  \\\n",
       "0          0  [-0.012635222636163235, 0.01425582729279995, -...   0   \n",
       "1          1  [-0.014502687379717827, 0.011701651848852634, ...   1   \n",
       "2          2  [-0.019268624484539032, 0.008453466929495335, ...   2   \n",
       "3          3  [-0.02202620729804039, -0.024142025038599968, ...   3   \n",
       "4          4  [-0.023361289873719215, -0.009505768306553364,...   4   \n",
       "\n",
       "                                                text  \\\n",
       "0  Page 1 of 2 Title: Adoption Assistance Departm...   \n",
       "1  Adoption Assistance BSWH. HR.BNFT.001.P Page 2...   \n",
       "2  sCOPE This document applies to the Baylor Scot...   \n",
       "3  qualifying incidents occurring after the emplo...   \n",
       "4  sCOPE This document applies to Baylor Scott & ...   \n",
       "\n",
       "                                            filename  \n",
       "0  AskBSWH/HR/Benefits/Policies & Procedures/Adop...  \n",
       "1  AskBSWH/HR/Benefits/Policies & Procedures/Adop...  \n",
       "2  AskBSWH/HR/Benefits/Policies & Procedures/Empl...  \n",
       "3  AskBSWH/HR/Benefits/Policies & Procedures/Empl...  \n",
       "4  AskBSWH/HR/Benefits/Policies & Procedures/Tuit...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the embeddings_array into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'vector_id': range(len(embeddings_array)),\n",
    "    'vector': embeddings_array\n",
    "})\n",
    "df[\"id\"] = df.index\n",
    "df[\"text\"] = chunks\n",
    "df[\"filename\"] = fnames\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47c6524e-35e0-43cd-963f-f83d7f0131c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client = SearchIndexClient(endpoint=search_service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"vector_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"text\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"filename\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"ask-vector-config\")\n",
    "]\n",
    "# Configure the vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"ask-vector-config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e88ae75-06bd-471c-8c67-c1bbbf0e486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askms-embeddings-index-v2 created\n"
     ]
    }
   ],
   "source": [
    "# Create the index \n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ce77a23-a018-40ea-98b0-1c3310a70301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'id' and 'vector_id' columns to string so one of them can serve as our key field  \n",
    "df['id'] = df['id'].astype(str)  \n",
    "df['vector_id'] = df['vector_id'].astype(str)\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries  \n",
    "documents = df.to_dict(orient='records')  \n",
    "# Split the data into chunks \n",
    "documents_chunks = [documents[i:i + 100] for i in range(0, len(documents), 100)]\n",
    "search_client = SearchClient(endpoint=search_service_endpoint, index_name=index_name, credential=credential)\n",
    "for chunk in documents_chunks:\n",
    "    result = search_client.upload_documents(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c208a-7646-4a0e-8ea5-2ecb89070e30",
   "metadata": {},
   "source": [
    "## Search embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deca3dea-8d81-49e2-ba73-2aa7cf7fa514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: protection. Help with out-of- pocket expenses when you or your family may needit the mos t.›C ost-effective. You can sign up for this coverage at economical group rates, which means you may payless for your coverage . ›C onvenient. We make it easy. No copays, deductibles, coinsurance or network requirementsto worry about and your insurance premiums can beeasily deducted from your pay check. ›P ortable. You may be able to take your coverage with you if you leave your employer – benefits won’tchange if you port your coverage. 2 Is there such a thing as too much protection? Only you can answer that question. Think about your expenses, savings and plans for the future. Then, consider this: ›E very 40 seconds , a stroke occurs in the U.S.3\n",
      "Source: AskBSWH/HR/Benefits/WebExtracts/Other Benefits/Accidental-Injury_Benefit-Summary.pdf\n",
      "Score: 0.84724617\n",
      "Text: protection. Help with out-of- pocket expenses when you or your family may needit the mos t.›C ost-effective. You can sign up for this coverage at economical group rates, which means you may payless for your coverage . ›C onvenient. We make it easy. No copays, deductibles, coinsurance or network requirementsto worry about and your insurance premiums can beeasily deducted from your pay check. ›P ortable. You may be able to take your coverage with you if you leave your employer – benefits won’tchange if you port your coverage. 2 Is there such a thing as too much protection? Only you can answer that question. Think about your expenses, savings and plans for the future. Then, consider this: ›E very 40 seconds , a stroke occurs in the U.S.3\n",
      "Source: AskBSWH/HR/Benefits/WebExtracts/Other Benefits/Critical-Illness_Benefit-Summary.pdf\n",
      "Score: 0.84724617\n",
      "Text: Prepare for the unexpected Life and AD&D insurance: We provide basic coverage for you at no cost, and you can elect additional coverage for yourself, your spouse and your children. Make sure to look at your beneficiary designations when reviewing this coverage. (pg. 27) Short-term disability (STD) insurance: You can elect 60% or 70% coverage for yourself that may pay a portion of your salary if you can’t work because of a qualified illness or injury or you become the parent of a new child. We automatically enroll you in coverage. If you do not want STD, you must opt out during your enrollment window. If you opt out when first eligible, future coverage may be subject to the pre-existing condition provision. (pg. 25) Keep in mind: You must be enrolled in STD coverage to receive parental leave benefits. Long-term disability (L TD) insurance: L TD may pay a benefit for a qualified illness or injury. We provide basic coverage to you at no cost. If you need additional income protection, a buy-up plan is available for purchase. (pg. 25) Start saving for retirement Y ou can enroll in the 401(k) retirement plan immediately by visiting BSWHRetirement.com. Elections can be changed at any time. Ready to enroll? 1. Log into MyPeoplePlace.com 2. Click Benefit Details tile 3. Click Benefit Enrollment 4. Click Start to access your enrollment event 5. Click each benefit tile to review and make your selections 6. Click Submit Enrollment to finalize your choicesPick health-related benefits Medical plans: SEQA/EQA, PPO or HSA (pg. 8) Dental: Cigna DHMO, Choice or Choice Plus ( pg. 11) Vision: EyeMed (pg. 12) Choose savings/spending account Healthcare accounts: Set aside pre-tax dollars to a spending/savings account to pay for eligible health expenses. Dependent Care FSA account: Pay for expenses related to child, elder and disabled adult care.** * Th e limited-purpose FSA can help pay for eligible dental and vision costs while you build a reserve in your HSA. ** Th e dependent care FSA is available to all eligible employees, whether enrolled in a medical plan or not. Supplement your coverage with voluntary benefits (pg. 15) A ccidental injury insurance C ritical illness insurance H ospital care insurance P repaid legal services General-purpose FSA Health savings account (HSA ) and/or limited- purpose FSA*SEQA/ EQA and PPO Plans HSA Plan 5 Benefits enrollmentBenefits checklist Use this handy checklist to help keep track of your benefits decisions.\n",
      "Source: AskBSWH/HR/Benefits/WebExtracts/AE/2023_Benefits-Guide_New-Hire.pdf\n",
      "Score: 0.8421208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pure Vector Search\n",
    "query = \"Who can i add to my coverage?\"\n",
    "  \n",
    "search_client = SearchClient(search_service_endpoint, index_name, AzureKeyCredential(search_service_api_key))  \n",
    "vector = Vector(value= generate_embeddings(query, model=EMBEDDING_DEPLOYMENT_NAME), k=3, fields=\"vector\")  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector],\n",
    ")\n",
    "cached = []\n",
    "  \n",
    "for result in results:\n",
    "    if result[\"@search.score\"] >= 0.81:\n",
    "        cached.append(result)\n",
    "        print(f\"Text: {result['text']}\")  \n",
    "        print(f\"Source: {result['filename']}\")\n",
    "        print(f\"Score: {result['@search.score']}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
